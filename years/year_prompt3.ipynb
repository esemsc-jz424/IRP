{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "447bc7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "import base64\n",
    "import shutil\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8fa81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Paths\n",
    "PDF_DIR = Path(\"pdf_folder\")  # Ensure this path contains your PDFs\n",
    "# OUTPUT_PATH = Path(\"results/classify_gpt_results1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aafba571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OCR è¡¥æ•‘å‡½æ•°ï¼šç”¨äºä»é¦–é¡µå’Œæœ«é¡µå›¾åƒæå–æ–‡æœ¬\n",
    "def extract_text_with_ocr(pdf_path, front_n=5, back_n=5, dpi=300):\n",
    "    import warnings\n",
    "    from pdf2image.exceptions import PDFPageCountError\n",
    "    try:\n",
    "        with tempfile.TemporaryDirectory() as path:\n",
    "            try:\n",
    "                images = convert_from_path(str(pdf_path), dpi=dpi, output_folder=path)\n",
    "            except PDFPageCountError as e:\n",
    "                return f\"OCR ERROR: PDF structure invalid â€“ {str(e)}\"\n",
    "            except Exception as e:\n",
    "                return f\"OCR ERROR: {str(e)}\"\n",
    "\n",
    "            total_pages = len(images)\n",
    "            if total_pages == 0:\n",
    "                return \"OCR ERROR: No images extracted\"\n",
    "\n",
    "            selected = images[:front_n] + images[-back_n:]\n",
    "            texts = []\n",
    "            for img in selected:\n",
    "                img = img.convert(\"L\")  # ç°åº¦å¢å¼º\n",
    "                text = pytesseract.image_to_string(img, lang=\"eng\")\n",
    "                texts.append(text)\n",
    "            return \"\\n\".join(texts)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"OCR ERROR (outer): {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1213328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_front_back_text(pdf_path, front_n=10, back_n=10, dpi=300):\n",
    "    try:\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        texts = []\n",
    "        for i in range(min(front_n, len(doc))):\n",
    "            texts.append(doc[i].get_text())\n",
    "        for i in range(max(0, len(doc) - back_n), len(doc)):\n",
    "            texts.append(doc[i].get_text())\n",
    "        doc.close()\n",
    "        full_text = \"\\n\".join(texts)\n",
    "        if len(full_text.strip()) < 100:\n",
    "            raise ValueError(\"Too short, fallback to OCR.\")\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Fallback to OCR on: {pdf_path.name} due to {str(e)}\")\n",
    "        return extract_text_with_ocr(pdf_path, front_n, back_n, dpi)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6f6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== 4. æ„é€  Prompt ==========\n",
    "def build_report_year_prompt(text):\n",
    "    return f\"\"\"\n",
    "You are an expert assistant helping to extract **the reporting period actually covered by the report** (the action period), not targets or future goals.\n",
    "\n",
    "Follow these rules strictly and return only a **JSON object** as described.\n",
    "\n",
    "---\n",
    "\n",
    "### What to EXTRACT (high priority cues)\n",
    "Pick the **most authoritative, explicit scope statement** such as lines containing:\n",
    "- \"covers ... from X to Y\"\n",
    "- \"reporting period:\" / \"report period:\"\n",
    "- \"for the year ended ...\" / \"for the period ended ...\"\n",
    "- \"fiscal year [range] from X to Y\" (when it states the scope of THIS report)\n",
    "- \"this report includes / contains data from X to Y\"\n",
    "\n",
    "### What to IGNORE (do NOT use as reporting period)\n",
    "- Long-term targets/roadmaps: e.g., \"by FY30\", \"by FY40\", \"from 2022 onward\", \"target reduction from FY2018 levels\"\n",
    "- Baseline/comparison references: \"compared to FY2018\", \"since 2019\", \"2019 highlights\"\n",
    "- Generic facts with years that do not declare the **covered period**\n",
    "- Multi-year strategies without explicit \"covers/reporting period\" verbs\n",
    "\n",
    "If multiple candidates exist, apply tie-breakers in this order:\n",
    "1) Prefer sentences that explicitly say \"covers/reporting period/for the year ended\".\n",
    "2) Prefer **the narrowest exact date/window** that clearly defines this reportâ€™s scope.\n",
    "3) Prefer statements that name **both start and end** (e.g., \"from Nov 1, 2019 to Oct 31, 2021\").\n",
    "4) If both a month/year range and a day/month/year range exist for the same scope, pick the **day-level** one.\n",
    "\n",
    "---\n",
    "\n",
    "### Normalization rules\n",
    "1) If it's a **single year**, like \"2013\" or \"FY2020\", return \"2013\" or \"2020\" (no months/days).\n",
    "2) If it's a **range of years only**, like \"2014â€“15\", return \"2014 to 2015\" (no months/days).\n",
    "3) If it includes **exact dates**, remove ordinals (1stâ†’1) and format as:\n",
    "   \"1 April 2020 to 31 March 2021\"\n",
    "4) If it uses **months only**:\n",
    "   \"April 2020 â€“ March 2021\" â†’ \"April 2020 to March 2021\"\n",
    "5) Convert dashes (â€“ or -) or slashes (/) to \"to\".\n",
    "6) Do **not** invent missing information.\n",
    "\n",
    "---\n",
    "\n",
    "### Output JSON\n",
    "\n",
    "If a valid reporting period is found:\n",
    "{{\n",
    "  \"normalized_report_year\": \"your final normalized version\",\n",
    "  \"original_expression\": \"verbatim span from the text that states the scope\",\n",
    "  \"source\": \"e.g. main text; table; footnote (if you can infer)\"\n",
    "}}\n",
    "\n",
    "If no valid reporting period is found:\n",
    "{{\n",
    "  \"normalized_report_year\": null,\n",
    "  \"original_expression\": null,\n",
    "  \"source\": \"NOT FOUND\"\n",
    "}}\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Mini example (the exact case you often miss)\n",
    "\n",
    "Input snippet:\n",
    "\"Here are a few facts... Pure is committing to a 3x reduction ... by FY30...  \n",
    "This inaugural report **covers ESG data, initiatives and activities from February 1, 2019 (FY20) to January 31, 2021 (FY21)**.\"\n",
    "\n",
    "Expected:\n",
    "{{\n",
    "  \"normalized_report_year\": \"1 February 2019 to 31 January 2021\",\n",
    "  \"original_expression\": \"covers ESG data, initiatives and activities from February 1, 2019 (FY20) to January 31, 2021 (FY21)\",\n",
    "  \"source\": \"main text\"\n",
    "}}\n",
    "\n",
    "---\n",
    "\n",
    "### Now analyze and extract:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# ========== 5. Vision æ¨¡å‹è¾…åŠ© ==========\n",
    "def encode_image_to_base64(pil_image):\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".png\") as f:\n",
    "        pil_image.save(f.name, format=\"PNG\")\n",
    "        with open(f.name, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def extract_year_from_vision(pdf_path, page_limit=3):\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=200)\n",
    "        for i, img in enumerate(images[:page_limit]):\n",
    "            b64 = encode_image_to_base64(img)\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": f\"Please extract the fiscal year or reporting period from page {i+1}. Please return in this JSON format:\\n\"\n",
    "                                                 \"{\\n  \\\"normalized_report_year\\\": \\\"...\\\",\\n  \\\"original_expression\\\": \\\"...\\\",\\n  \\\"source\\\": \\\"Page {i+1}, image-based\\\"\\n}\"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{b64}\"}}\n",
    "                    ]}\n",
    "                ],\n",
    "                max_tokens=300\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            parsed = json.loads(content.strip()) if content.strip().startswith(\"{\") else eval(content.strip())\n",
    "            if parsed.get(\"normalized_report_year\"):\n",
    "                return parsed\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"normalized_report_year\": None,\n",
    "            \"original_expression\": None,\n",
    "            \"source\": f\"Vision ERROR: {e}\"\n",
    "        }\n",
    "    return {\n",
    "        \"normalized_report_year\": None,\n",
    "        \"original_expression\": None,\n",
    "        \"source\": \"Vision NOT FOUND\"\n",
    "    }\n",
    "\n",
    "# ========== 6. ä¸»å‡½æ•°ï¼šå…ˆæ–‡æœ¬ï¼Œå† Vision ==========\n",
    "def extract_report_year(pdf_path):\n",
    "    try:\n",
    "        text = extract_front_back_text(pdf_path)\n",
    "        prompt = build_report_year_prompt(text)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        parsed = json.loads(content.strip()) if content.strip().startswith(\"{\") else eval(content.strip())\n",
    "        if parsed.get(\"normalized_report_year\"):\n",
    "            return parsed\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ GPT-4.1-mini failed on {Path(pdf_path).name}, fallback to Vision...\")\n",
    "    return extract_year_from_vision(pdf_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5795dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path):\n",
    "    text = extract_front_back_text(pdf_path)\n",
    "\n",
    "    # report_type, sustainability,sustainability_name,classify_reasoning = classify_report_type(text)\n",
    "    year = extract_report_year(pdf_path)\n",
    "    # company = extract_company_or_publisher_with_gpt(text)\n",
    "\n",
    "    return {\n",
    "        \"filename\": pdf_path.name,\n",
    "        # \"report_type\": rtype.get(\"report_type\"),\n",
    "        # \"has_sustainability_section\": rtype.get(\"has_sustainability_section\"),\n",
    "        # \"sustainability_section_name\": rtype.get(\"sustainability_section_name\"),\n",
    "\n",
    "        \"normalized_report_year\": year.get(\"normalized_report_year\"),\n",
    "        \"original_expression\": year.get(\"original_expression\"),\n",
    "        \"year_source\": year.get(\"source\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea4004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   0%|          | 0/1278 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Unknown_8f57f855-11bb-496d-9916-91ff88cb537b_sqi6k2d1.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Unknown_2020_SEBANG20SUSTAINABILITY20REPORT_ENG_cty885ga.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Knoll_Inc_Knoll_Enviro_2008_gqetdkb7.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Toyota_Industries_Corp_environment2004_40h96hjb.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Intel_Corp__fwws0wtm.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   0%|          | 1/1278 [00:47<16:52:17, 47.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Shui_on_Land_Ltd_3_7xr7l35i.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   0%|          | 4/1278 [00:54<3:51:27, 10.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Unknown_637407834879900000_opfazoiq.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Newport_Corp_Volvo-Ocean-Race-Newport-Stopover-Sustainability-Report_2a7nj4kv.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Mint_Corp_The_mint_ar_10_eng_final_jsnlk95f.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Unknown_2015-201620Sustainability-Report20210825_171509627_x9kgqjf9.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   1%|          | 9/1278 [01:27<2:07:53,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Isabella_Bank_Corp_COMBO_494873_tqto1pwh.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on TIM_SA_TIM-2022-sustainability-report-ENG_5qx6prh7.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Unknown_2021_RPM_Sustainability_Report_i4bfnpju.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   1%|          | 10/1278 [01:44<3:11:43,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Unknown_2022060100165_a05pfov4.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   1%|          | 11/1278 [02:01<3:55:37, 11.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on BBVA_Compass_Bancshares_Inc_Informe_anual_2010_Eng_tcm927-346446_3qyfos2v.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Sundaram_Clayton_Ltd_BusinessResponsibilityReport2021-22_2njbf6an.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   1%|          | 13/1278 [02:20<3:21:46,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Citrix_Systems_Inc_citrix-sustainability-report-2020_qnsobgyv.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   1%|â–         | 16/1278 [02:36<2:16:24,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Sterlite_Technologies_Ltd_Annual-report-2016_p4sp9tbo.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Unknown_2022053120034016302_1gl1sbq2.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   1%|â–         | 17/1278 [02:50<3:01:05,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on India_Glycols_Ltd_annual-report-2019-20_g3mx3rps.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on JSW_Energy_Ltd_Integrated20Annual20Report202021_wgoux4h5.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Sojitz_Corporation_csr2007_all_3ucpks58.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   2%|â–         | 20/1278 [03:14<2:40:24,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on PerkinElmer_Inc_2016-2017-Corporate-Social-Responsibility-Report_tcm137-198142_lsp07rq5.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Appian_Corp_476815272_Social-Responsibility-Doc_final-7_chc5albm.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on IQE_PLC__12224-iqe-annual-report-2018_web_10fo3ave.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Hirose_Electric_Co_Ltd_HiroseElectric_SustainabilityReport2020E_3ixvbvy1.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   2%|â–         | 24/1278 [03:48<2:05:53,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Asian_Paints_Ltd_q4_3gbbkfbt.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   2%|â–         | 26/1278 [04:02<2:11:34,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Metair_Investment_Ltd_Metair-IAR_2020_d28upgn8.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Ambuja_Cements_Ltd_Sustainable-Development-Report-2016_8ixcsb2n.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Marks_and_Spencer_Group_PLC_OTC_MAKSF_2012_ffk4yhuq.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   2%|â–         | 27/1278 [04:16<2:53:06,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Chambal_Fertilisers__Chemicals_Gadepan-I-And-II-Compliance-Report_es33maw9.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Unknown_2020-sustainability-report_6rybl2c1.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on GL_Ltd_kilroy-realty-corporation-sustainability-report-2018_cnh296bj.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Unknown_2020-annual-report-final-copy_q50a286f.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   3%|â–         | 32/1278 [04:57<2:23:02,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Hess_Corp_hess-2021-sustainability-report_n2wei7r5.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Unknown_124096-19In-26060160T18678681728S-In_eolydoef.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   3%|â–         | 34/1278 [05:13<2:21:53,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Gresham_Technologies_PLC_LSE_GHTL_2019_biqvttuq.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   3%|â–         | 35/1278 [05:22<2:30:51,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Starbucks_Corp_4dd6216d0fd0400f8689eceba0497e04_zovctl4r.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Unknown_95F8F73558174EF49F0B2B278C2F2D27_3tevpoql.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on LyondellBasell_Industries_NV_2019_sustainability_report_fvt1o08m.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   3%|â–         | 38/1278 [05:49<2:28:30,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Unknown_27464_7y8uu0oh.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   3%|â–         | 39/1278 [06:01<2:57:05,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on KRBL_Ltd_KRBL_ANNUAL_REPORT_2018-19_5wnor8by.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Singapore_Airlines_Ltd_annualreport0910_xfiqnr8w.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing PDFs:   3%|â–         | 39/1278 [06:06<3:14:15,  9.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®è·¯å¾„\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "pdf_dir = Path(\"pdf_folder\")\n",
    "output_path = Path(\"years/report_years2.csv\")\n",
    "os.makedirs(output_path.parent, exist_ok=True)\n",
    "\n",
    "# æ”¶é›†æ‰€æœ‰ PDF æ–‡ä»¶\n",
    "pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "\n",
    "# å®šä¹‰æœ€å¤§çº¿ç¨‹æ•°ï¼ˆæ¨è 5â€“10ï¼‰\n",
    "MAX_WORKERS = 5\n",
    "\n",
    "# ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    results = list(tqdm(executor.map(process_pdf, pdf_files), total=len(pdf_files), desc=\"ğŸš€ Processing PDFs\"))\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"âœ… Done! Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a793833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… number of combine samplesï¼š224\n",
      "âœ… correct matchesï¼š185\n",
      "âœ… Fuzzy Accuracyï¼š82.59%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === 1. åŠ è½½æ•°æ® ===\n",
    "gpt_df = pd.read_csv(\"years/report_years2.csv\")  # GPTæå–ç»“æœ\n",
    "label_df = pd.read_excel(\"check/rfyear_annotation2.xlsx\")            # æ ‡æ³¨ç»“æœ\n",
    "\n",
    "# === 2. é‡å‘½ååˆ—å¯¹é½ã€åˆå¹¶ï¼ˆæŒ‰æ–‡ä»¶åï¼‰===\n",
    "gpt_df.rename(columns={\"filename\": \"pdf_name\", \"report_year\": \"normalized_report_year\"}, inplace=True)\n",
    "merged = pd.merge(label_df, gpt_df, on=\"pdf_name\", how=\"inner\")\n",
    "\n",
    "# === 3. æ¸…æ´—æ–‡æœ¬ï¼šç©ºå€¼è½¬ç©ºä¸²ï¼Œå¤§å°å†™ã€ç©ºæ ¼ã€æ¢è¡Œå¤„ç† ===\n",
    "def clean_text(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    return str(s).strip().lower().replace(\"\\n\", \" \")\n",
    "\n",
    "merged[\"normalized_report_year\"] = merged[\"normalized_report_year\"].apply(clean_text)\n",
    "merged[\"chosen_rfyear\"] = merged[\"chosen_rfyear\"].apply(clean_text)\n",
    "\n",
    "# === 4. å®šä¹‰è¾…åŠ©å‡½æ•° ===\n",
    "import re\n",
    "\n",
    "def normalize_year_text(s):\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.lower().strip()\n",
    "    \n",
    "    # æ›¿æ¢å¸¸è§è¿å­—ç¬¦ä¸ºç»Ÿä¸€æ ¼å¼\n",
    "    s = s.replace(\"â€“\", \" to \").replace(\"-\", \" to \").replace(\"/\", \" to \")\n",
    "\n",
    "    # æ¸…é™¤æ— æ•ˆå­—ç¬¦\n",
    "    s = re.sub(r'[\\(\\)\\[\\],;:]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "\n",
    "    # fy ç¼©å†™å¤„ç†ï¼šfy2020 æˆ– fy 2020 â†’ 2020\n",
    "    s = re.sub(r'\\bfy\\s*(\\d{4})\\b', r'\\1', s)                   # fy 2020 â†’ 2020\n",
    "    s = re.sub(r'\\bfy\\s*(\\d{2})\\b', lambda m: f\"20{m.group(1)}\", s)  # fy 19 â†’ 2019\n",
    "\n",
    "    # åŒºé—´æ ¼å¼å¤„ç†ï¼š2020 to 21 â†’ 2020 to 2021\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s+to\\s+(\\d{2})\\b', lambda m: f\"{m.group(1)} to 20{m.group(2)}\", s)\n",
    "\n",
    "    # åŒºé—´æ ¼å¼å¤„ç†ï¼š2020â€“2021ã€2020-2021ã€fy2020 to 2021 â†’ 2020 to 2021\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s*to\\s*(20\\d{2})\\b', r'\\1 to \\2', s)\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s+to\\s+20\\d{2}', r'\\g<0>', s)\n",
    "\n",
    "    # æ¸…ç†åºæ•°è¯\n",
    "    s = re.sub(r'\\b(\\d{1,2})(st|nd|rd|th)\\b', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "def extract_years(s):\n",
    "    return sorted(set(re.findall(r'\\b(20\\d{2}|19\\d{2})\\b', s)))\n",
    "\n",
    "def is_fuzzy_match(a, b):\n",
    "    a_norm = normalize_year_text(a)\n",
    "    b_norm = normalize_year_text(b)\n",
    "    if a_norm == b_norm:\n",
    "        return True\n",
    "\n",
    "    # æå–å¹´ä»½é›†åˆ\n",
    "    a_years = extract_years(a_norm)\n",
    "    b_years = extract_years(b_norm)\n",
    "\n",
    "    if not a_years or not b_years:\n",
    "        return False\n",
    "\n",
    "    # æ’åºåå†æ¯”è¾ƒ\n",
    "    a_sorted = sorted(set(a_years))\n",
    "    b_sorted = sorted(set(b_years))\n",
    "\n",
    "    # å®Œå…¨ä¸€è‡´æˆ–åŒ…å«\n",
    "    if a_sorted == b_sorted:\n",
    "        return True\n",
    "    if len(a_sorted) == 1 and a_sorted[0] in b_sorted:\n",
    "        return True\n",
    "    if len(b_sorted) == 1 and b_sorted[0] in a_sorted:\n",
    "        return True\n",
    "\n",
    "    # ä»…ä¸€ä¸ªå¹´ä»½æ—¶å…è®¸Â±1\n",
    "    if len(a_sorted) == 1 and len(b_sorted) == 1 and abs(int(a_sorted[0]) - int(b_sorted[0])) <= 1:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# === 5. åº”ç”¨æ¨¡ç³ŠåŒ¹é…å‡½æ•° ===\n",
    "merged[\"fuzzy_match\"] = merged.apply(lambda row: is_fuzzy_match(row[\"normalized_report_year\"], row[\"chosen_rfyear\"]), axis=1)\n",
    "\n",
    "# === 6. è®¡ç®—å‡†ç¡®ç‡ ===\n",
    "total = len(merged)\n",
    "correct = merged[\"fuzzy_match\"].sum()\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"âœ… number of combine samplesï¼š{total}\")\n",
    "print(f\"âœ… correct matchesï¼š{correct}\")\n",
    "print(f\"âœ… Fuzzy Accuracyï¼š{accuracy:.2%}\")\n",
    "\n",
    "# === 7. ä¿å­˜ç»“æœ ===\n",
    "# merged[[\"pdf_name\", \"chosen_rfyear\", \"normalized_report_year\", \"fuzzy_match\"]].to_csv(\"eval/report_years_comparison1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231e7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… number of combine samplesï¼š221\n",
      "âœ… correct matchesï¼š187\n",
      "âœ… Fuzzy Accuracyï¼š84.62%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === 1. åŠ è½½æ•°æ® ===\n",
    "gpt_df = pd.read_csv(\"years/report_years2.csv\")  # GPTæå–ç»“æœ\n",
    "label_df = pd.read_excel(\"check/rfyear_annotation2.xlsx\")            # æ ‡æ³¨ç»“æœ\n",
    "\n",
    "# === 2. é‡å‘½ååˆ—å¯¹é½ã€åˆå¹¶ï¼ˆæŒ‰æ–‡ä»¶åï¼‰===\n",
    "gpt_df.rename(columns={\"filename\": \"pdf_name\", \"report_year\": \"normalized_report_year\"}, inplace=True)\n",
    "merged = pd.merge(label_df, gpt_df, on=\"pdf_name\", how=\"inner\")\n",
    "\n",
    "# === 3. æ¸…æ´—æ–‡æœ¬ï¼šç©ºå€¼è½¬ç©ºä¸²ï¼Œå¤§å°å†™ã€ç©ºæ ¼ã€æ¢è¡Œå¤„ç† ===\n",
    "def clean_text(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    return str(s).strip().lower().replace(\"\\n\", \" \")\n",
    "\n",
    "merged[\"normalized_report_year\"] = merged[\"normalized_report_year\"].apply(clean_text)\n",
    "merged[\"chosen_rfyear\"] = merged[\"chosen_rfyear\"].apply(clean_text)\n",
    "\n",
    "# === 4. å®šä¹‰è¾…åŠ©å‡½æ•° ===\n",
    "import re\n",
    "\n",
    "def normalize_year_text(s):\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.lower().strip()\n",
    "    \n",
    "    # æ›¿æ¢å¸¸è§è¿å­—ç¬¦ä¸ºç»Ÿä¸€æ ¼å¼\n",
    "    s = s.replace(\"â€“\", \" to \").replace(\"-\", \" to \").replace(\"/\", \" to \")\n",
    "\n",
    "    # æ¸…é™¤æ— æ•ˆå­—ç¬¦\n",
    "    s = re.sub(r'[\\(\\)\\[\\],;:]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "\n",
    "    # fy ç¼©å†™å¤„ç†ï¼šfy2020 æˆ– fy 2020 â†’ 2020\n",
    "    s = re.sub(r'\\bfy\\s*(\\d{4})\\b', r'\\1', s)                   # fy 2020 â†’ 2020\n",
    "    s = re.sub(r'\\bfy\\s*(\\d{2})\\b', lambda m: f\"20{m.group(1)}\", s)  # fy 19 â†’ 2019\n",
    "\n",
    "    # åŒºé—´æ ¼å¼å¤„ç†ï¼š2020 to 21 â†’ 2020 to 2021\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s+to\\s+(\\d{2})\\b', lambda m: f\"{m.group(1)} to 20{m.group(2)}\", s)\n",
    "\n",
    "    # åŒºé—´æ ¼å¼å¤„ç†ï¼š2020â€“2021ã€2020-2021ã€fy2020 to 2021 â†’ 2020 to 2021\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s*to\\s*(20\\d{2})\\b', r'\\1 to \\2', s)\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s+to\\s+20\\d{2}', r'\\g<0>', s)\n",
    "\n",
    "    # æ¸…ç†åºæ•°è¯\n",
    "    s = re.sub(r'\\b(\\d{1,2})(st|nd|rd|th)\\b', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "def extract_years(s):\n",
    "    return sorted(set(re.findall(r'\\b(20\\d{2}|19\\d{2})\\b', s)))\n",
    "\n",
    "def is_fuzzy_match(a, b):\n",
    "    a_norm = normalize_year_text(a)\n",
    "    b_norm = normalize_year_text(b)\n",
    "    if a_norm == b_norm:\n",
    "        return True\n",
    "\n",
    "    # æå–å¹´ä»½é›†åˆ\n",
    "    a_years = extract_years(a_norm)\n",
    "    b_years = extract_years(b_norm)\n",
    "\n",
    "    if not a_years or not b_years:\n",
    "        return False\n",
    "\n",
    "    # æ’åºåå†æ¯”è¾ƒ\n",
    "    a_sorted = sorted(set(a_years))\n",
    "    b_sorted = sorted(set(b_years))\n",
    "\n",
    "    # å®Œå…¨ä¸€è‡´æˆ–åŒ…å«\n",
    "    if a_sorted == b_sorted:\n",
    "        return True\n",
    "    if len(a_sorted) == 1 and a_sorted[0] in b_sorted:\n",
    "        return True\n",
    "    if len(b_sorted) == 1 and b_sorted[0] in a_sorted:\n",
    "        return True\n",
    "\n",
    "    # ä»…ä¸€ä¸ªå¹´ä»½æ—¶å…è®¸Â±1\n",
    "    if len(a_sorted) == 1 and len(b_sorted) == 1 and abs(int(a_sorted[0]) - int(b_sorted[0])) <= 1:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# === 5. åº”ç”¨æ¨¡ç³ŠåŒ¹é…å‡½æ•° ===\n",
    "merged[\"fuzzy_match\"] = merged.apply(lambda row: is_fuzzy_match(row[\"normalized_report_year\"], row[\"chosen_rfyear\"]), axis=1)\n",
    "\n",
    "# === 6. è®¡ç®—å‡†ç¡®ç‡ ===\n",
    "total = len(merged)\n",
    "correct = merged[\"fuzzy_match\"].sum()\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"âœ… number of combine samplesï¼š{total}\")\n",
    "print(f\"âœ… correct matchesï¼š{correct}\")\n",
    "print(f\"âœ… Fuzzy Accuracyï¼š{accuracy:.2%}\")\n",
    "\n",
    "# === 7. ä¿å­˜ç»“æœ ===\n",
    "# merged[[\"pdf_name\", \"chosen_rfyear\", \"normalized_report_year\", \"fuzzy_match\"]].to_csv(\"eval/2report_years_comparison1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2ea82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irp_pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
