{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3280ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:   6%|â–‹         | 82/1277 [01:21<19:22,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Armstrong_Flooring_Inc_SustainabilityReport-2020_kot54emv.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:   7%|â–‹         | 84/1277 [01:21<13:01,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Arvind_Ltd_Arvind_AR_2022-23_0_iwp4673c.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:   9%|â–‰         | 115/1277 [01:50<12:15,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on BASF_SE_2012_BASF_Report_lmq79gwn.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  12%|â–ˆâ–        | 158/1277 [02:41<20:24,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Boryung_Corporation_EBB3B4EBA0B920ECA780EC868DEAB080EB8AA5EAB2BDEC9881EBB3B4EAB3A0EC849CEC9881EBACB8_ebpit5lz.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  15%|â–ˆâ–        | 190/1277 [03:16<21:49,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on COSCO_SHIPPING_Energy_Transportation_Co_Ltd_quality2022en_itv517g2.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  19%|â–ˆâ–‰        | 240/1277 [04:32<18:48,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Columbia_Banking_System_Inc__-Old_Annual20Progress20Report202022-23_huqi1n89.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  23%|â–ˆâ–ˆâ–       | 290/1277 [05:53<27:54,  1.70s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on EKI_Energy_Services_Limited_69298543284_zj7y1tjh.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  25%|â–ˆâ–ˆâ–       | 316/1277 [06:21<19:11,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Essential_Utilities_Inc_34ecde9c-8434-4849-a903-48114b59c681_f5v52887.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  32%|â–ˆâ–ˆâ–ˆâ–      | 406/1277 [08:13<13:31,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Hansae_Yes24_Holdings_Co_Ltd_HANSAE20YES2420HOLDINGS20ESG20REPORT202022_th5kzsfk.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  34%|â–ˆâ–ˆâ–ˆâ–      | 434/1277 [08:45<16:20,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Home_Inns__Hotels_Management_Inc_Barclays_Bank_PLC_Annual_Report_202014_5lj1epic.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  34%|â–ˆâ–ˆâ–ˆâ–      | 440/1277 [08:51<16:08,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Hyosung_Corp_SR_2020_en_8g98j6gk.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 920/1277 [18:43<06:51,  1.15s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Tam_Jai_International_Co_Ltd_2022083101184_go5rbp4a.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 992/1277 [20:06<05:57,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Unknown_0720_FAO_Shifting_cultivation_livelihoodfood_security_j6g4r7xy.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1014/1277 [20:40<04:48,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Unknown_1901_4ch3st16.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1040/1277 [21:06<03:04,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Unknown_2014SustainRpt_FNL_lr_7mrwsfm7.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1174/1277 [23:38<01:20,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Unknown_2023042101335_kyzhtmjn.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1192/1277 [24:04<02:30,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Unknown_23076_Whitbread_AR2020_web_0v2mxh4f.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1274/1277 [26:17<00:04,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ GPT-4.1-mini failed on Unknown_adbi-managing-transition-low-carbon-economy_087is5zy.pdf, fallback to Vision...\n",
      "âš ï¸ GPT-4.1-mini failed on Unknown_adp07-sus-fr_95qx6prh.pdf, fallback to Vision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing PDFs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1277/1277 [26:17<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Extraction complete! Results saved to: results/extracted_report_years_formal1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import openai\n",
    "import pytesseract\n",
    "import tempfile\n",
    "import base64\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "\n",
    "# ========== 1. åˆå§‹åŒ– OpenAI ==========\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ========== 2. æå–æ–‡æœ¬ï¼šå‰å20é¡µ ==========\n",
    "def extract_front_back_text(pdf_path, front_n=10, back_n=10):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        texts = [doc[i].get_text() for i in range(min(front_n, len(doc)))]\n",
    "        texts += [doc[i].get_text() for i in range(max(0, len(doc) - back_n), len(doc))]\n",
    "        doc.close()\n",
    "        full_text = \"\\n\".join(texts)\n",
    "        if len(full_text.strip()) < 50:\n",
    "            raise ValueError(\"Too little text\")\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ PyMuPDF failed on {pdf_path.name}, switching to OCR...\")\n",
    "        return extract_text_with_ocr(pdf_path)\n",
    "\n",
    "# ========== 3. OCR è¡¥æ•‘ ==========\n",
    "def extract_text_with_ocr(pdf_path, dpi=300):\n",
    "    with tempfile.TemporaryDirectory() as path:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi, output_folder=path)\n",
    "        text_parts = [pytesseract.image_to_string(img) for img in images[:3] + images[-3:]]\n",
    "        return \"\\n\".join(text_parts)\n",
    "\n",
    "# ========== 4. æ„é€  Prompt ==========\n",
    "def build_report_year_prompt(text):\n",
    "    return f\"\"\"\n",
    "You are an expert assistant helping to extract reporting years from corporate reports.\n",
    "\n",
    "Please complete the following tasks after carefully reading the text below:\n",
    "\n",
    "---\n",
    "\n",
    "### Task 1: Identify any **fiscal year**, **reporting period**, or **financial year** expressions.\n",
    "\n",
    "Common examples include:\n",
    "- \"for the year ended 31 December 2021\"\n",
    "- \"reporting period: April 2021 â€“ March 2022\"\n",
    "- \"2011â€“14\" or \"FY2017\"\n",
    "- \"2020 ESG highlights\"\n",
    "- \"2013 Corporate Report\"\n",
    "\n",
    "---\n",
    "\n",
    "### Task 2: Normalize each valid expression into a standardized format:\n",
    "\n",
    "1. If the expression is a **single year**, like `\"2013\"` or `\"FY2020\"`, output `\"2013\"` or `\"2020\"` as-is.\n",
    "2. If it is a **range of years only**, like `\"2014â€“15\"`, output `\"2014 to 2015\"`. Do **not** add months or days.\n",
    "3. If full dates are mentioned (e.g., `\"1st April 2020 â€“ 31st March 2021\"`), remove ordinal suffixes (`1st`, `2nd`, etc.) and format as:\n",
    "   `\"1 April 2020 to 31 March 2021\"`\n",
    "4. Always convert dashes (\"â€“\" or \"-\") to \"to\".\n",
    "5. If month is provided but not day (e.g., `\"April 2020 â€“ March 2021\"`), convert to `\"April 2020 to March 2021\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### Task 3: Return a JSON in the following format:\n",
    "\n",
    "If a valid reporting year expression is found:\n",
    "\n",
    "{{\n",
    "  \"normalized_report_year\": \"your normalized result here\",\n",
    "  \"original_expression\": \"copy the original matched expression\",\n",
    "  \"source\": \"e.g. Page 1, main text\"\n",
    "}}\n",
    "\n",
    "If no valid expression is found:\n",
    "\n",
    "{{\n",
    "  \"normalized_report_year\": null,\n",
    "  \"original_expression\": null,\n",
    "  \"source\": \"NOT FOUND\"\n",
    "}}\n",
    "\n",
    "Only output the JSON object. Do not add explanations or commentary.\n",
    "\n",
    "---\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# ========== 5. Vision æ¨¡å‹è¾…åŠ© ==========\n",
    "def encode_image_to_base64(pil_image):\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".png\") as f:\n",
    "        pil_image.save(f.name, format=\"PNG\")\n",
    "        with open(f.name, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def extract_year_from_vision(pdf_path, client, page_limit=3):\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=200)\n",
    "        for i, img in enumerate(images[:page_limit]):\n",
    "            b64 = encode_image_to_base64(img)\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": f\"Please extract the fiscal year or reporting period from page {i+1}. Please return in this JSON format:\\n\"\n",
    "                                                 \"{\\n  \\\"normalized_report_year\\\": \\\"...\\\",\\n  \\\"original_expression\\\": \\\"...\\\",\\n  \\\"source\\\": \\\"Page {i+1}, image-based\\\"\\n}\"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{b64}\"}}\n",
    "                    ]}\n",
    "                ],\n",
    "                max_tokens=300\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            parsed = json.loads(content.strip()) if content.strip().startswith(\"{\") else eval(content.strip())\n",
    "            if parsed.get(\"normalized_report_year\"):\n",
    "                return parsed\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"normalized_report_year\": None,\n",
    "            \"original_expression\": None,\n",
    "            \"source\": f\"Vision ERROR: {e}\"\n",
    "        }\n",
    "    return {\n",
    "        \"normalized_report_year\": None,\n",
    "        \"original_expression\": None,\n",
    "        \"source\": \"Vision NOT FOUND\"\n",
    "    }\n",
    "\n",
    "# ========== 6. ä¸»å‡½æ•°ï¼šå…ˆæ–‡æœ¬ï¼Œå† Vision ==========\n",
    "def extract_report_year(pdf_path, client):\n",
    "    try:\n",
    "        text = extract_front_back_text(pdf_path)\n",
    "        prompt = build_report_year_prompt(text)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        parsed = json.loads(content.strip()) if content.strip().startswith(\"{\") else eval(content.strip())\n",
    "        if parsed.get(\"normalized_report_year\"):\n",
    "            return parsed\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ GPT-4.1-mini failed on {Path(pdf_path).name}, fallback to Vision...\")\n",
    "    return extract_year_from_vision(pdf_path, client)\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# å¤šçº¿ç¨‹å¤„ç†å‡½æ•°\n",
    "def process_pdf(pdf_path):\n",
    "    try:\n",
    "        out = extract_report_year(str(pdf_path), client)\n",
    "        out[\"filename\"] = pdf_path.name\n",
    "    except Exception as e:\n",
    "        out = {\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"normalized_report_year\": None,\n",
    "            \"original_expression\": None,\n",
    "            \"source\": f\"ERROR: {e}\"\n",
    "        }\n",
    "    return out\n",
    "\n",
    "def batch_extract_years_multithread(pdf_dir, output_csv=\"results/extracted_report_years_formal1.csv\", n_jobs=2):\n",
    "    pdf_dir = Path(pdf_dir)\n",
    "    pdf_files = sorted(pdf_dir.glob(\"*.pdf\"))\n",
    "    os.makedirs(Path(output_csv).parent, exist_ok=True)\n",
    "\n",
    "    # âœ… å¹¶è¡Œå¤„ç†\n",
    "    results = Parallel(n_jobs=n_jobs, prefer=\"threads\")(\n",
    "        delayed(process_pdf)(pdf) for pdf in tqdm(pdf_files, desc=\"ğŸ“„ Processing PDFs\")\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nâœ… Extraction complete! Results saved to: {output_csv}\")\n",
    "\n",
    "# ========== 8. è¿è¡Œ ==========\n",
    "if __name__ == \"__main__\":\n",
    "    batch_extract_years_multithread(\"pdf_folder\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986b71c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²ä¿å­˜ç®€æ´å¯¹æ¯”ç»“æœä¸º rfyear_comparison_results_minimal.csv\n",
      "ğŸ“Š åŒ¹é…æƒ…å†µç»Ÿè®¡ï¼š\n",
      "match_result\n",
      "missing     389\n",
      "mismatch    115\n",
      "match        97\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è¯»å–æ–‡ä»¶\n",
    "anno_df = pd.read_excel(\"check/rfyear_annotation.xlsx\")\n",
    "result_df = pd.read_csv(\"results/extracted_report_years_formal1.csv\")\n",
    "\n",
    "# æ¸…æ´—æ–‡ä»¶ååˆ—\n",
    "anno_df[\"pdf_name_clean\"] = anno_df[\"pdf_name\"].str.strip().str.lower()\n",
    "result_df[\"filename_clean\"] = result_df[\"filename\"].str.strip().str.lower()\n",
    "\n",
    "# åˆå¹¶ä¸¤ä¸ªè¡¨\n",
    "merged_df = pd.merge(\n",
    "    anno_df,\n",
    "    result_df,\n",
    "    left_on=\"pdf_name_clean\",\n",
    "    right_on=\"filename_clean\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# å¹´ä»½å¯¹æ¯”å‡½æ•°\n",
    "def match_year(human, gpt):\n",
    "    if pd.isna(human) or pd.isna(gpt):\n",
    "        return \"missing\"\n",
    "    return \"match\" if str(human).strip().lower() == str(gpt).strip().lower() else \"mismatch\"\n",
    "\n",
    "# æ·»åŠ åŒ¹é…ç»“æœ\n",
    "merged_df[\"match_result\"] = merged_df.apply(\n",
    "    lambda row: match_year(row[\"chosen_rfyear\"], row[\"normalized_report_year\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ä»…ä¿ç•™æœ‰ç”¨å­—æ®µ\n",
    "final_df = merged_df[[\n",
    "    \"pdf_name\", \"chosen_rfyear\", \"normalized_report_year\", \"match_result\"\n",
    "]]\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "# final_df.to_csv(\"rfyear_comparison_results_minimal.csv\", index=False)\n",
    "\n",
    "# è¾“å‡ºç»Ÿè®¡\n",
    "print(\"âœ… å·²ä¿å­˜ç®€æ´å¯¹æ¯”ç»“æœä¸º rfyear_comparison_results_minimal.csv\")\n",
    "print(\"ğŸ“Š åŒ¹é…æƒ…å†µç»Ÿè®¡ï¼š\")\n",
    "print(final_df[\"match_result\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa2420e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆå¹¶æ ·æœ¬æ•°ï¼š224\n",
      "âœ… åŒ¹é…æ­£ç¡®æ•°ï¼š97\n",
      "âœ… å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ï¼š43.30%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 1. è¯»å–ä¸¤ä¸ªæ–‡ä»¶\n",
    "gpt_df = pd.read_csv(\"results/extracted_report_years_formal1.csv\")  # æå–ç»“æœ\n",
    "label_df = pd.read_excel(\"check/rfyear_annotation.xlsx\")         # äººå·¥æ ‡æ³¨æ–‡ä»¶\n",
    "\n",
    "# 2. é‡å‘½ååˆ—ä»¥ä¾¿å¯¹é½åˆå¹¶\n",
    "gpt_df.rename(columns={\"filename\": \"pdf_name\", \"report_year\": \"normalized_report_year\"}, inplace=True)\n",
    "\n",
    "# 3. åˆå¹¶ä¸¤ä¸ªè¡¨ï¼ˆinner joinï¼Œåªä¿ç•™ä¸¤ä¸ªéƒ½æœ‰çš„ pdfï¼‰\n",
    "merged = pd.merge(label_df, gpt_df, on=\"pdf_name\", how=\"inner\")\n",
    "\n",
    "# 4. æ ‡å‡†åŒ–å­—ç¬¦ä¸²æ ¼å¼ï¼ˆå»é™¤ç©ºæ ¼å¤§å°å†™ç­‰ï¼‰\n",
    "merged[\"normalized_report_year\"] = merged[\"normalized_report_year\"].astype(str).str.strip().str.lower()\n",
    "merged[\"chosen_rfyear\"] = merged[\"chosen_rfyear\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# 5. å®šä¹‰åŒ¹é…é€»è¾‘ï¼ˆå®Œå…¨åŒ¹é…å³å¯ï¼‰\n",
    "merged[\"match\"] = merged[\"normalized_report_year\"] == merged[\"chosen_rfyear\"]\n",
    "\n",
    "# 6. ç»Ÿè®¡æŒ‡æ ‡\n",
    "total = len(merged)\n",
    "correct = merged[\"match\"].sum()\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"âœ… åˆå¹¶æ ·æœ¬æ•°ï¼š{total}\")\n",
    "print(f\"âœ… åŒ¹é…æ­£ç¡®æ•°ï¼š{correct}\")\n",
    "print(f\"âœ… å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ï¼š{accuracy:.2%}\")\n",
    "\n",
    "# 7. å¯é€‰ï¼šä¿å­˜å¯¹æ¯”ç»“æœ\n",
    "merged[[\"pdf_name\", \"chosen_rfyear\", \"normalized_report_year\", \"match\"]].to_csv(\"eval/year_comparison1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f0a0901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆå¹¶æ ·æœ¬æ•°ï¼š224\n",
      "âœ… åŒ¹é…æ­£ç¡®æ•°ï¼š178\n",
      "âœ… æ¨¡ç³ŠåŒ¹é…å‡†ç¡®ç‡ï¼ˆFuzzy Accuracyï¼‰ï¼š79.46%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === 1. åŠ è½½æ•°æ® ===\n",
    "gpt_df = pd.read_csv(\"results/extracted_report_years_formal1.csv\")  # GPTæå–ç»“æœ\n",
    "label_df = pd.read_excel(\"check/rfyear_annotation.xlsx\")            # æ ‡æ³¨ç»“æœ\n",
    "\n",
    "# === 2. é‡å‘½ååˆ—å¯¹é½ã€åˆå¹¶ï¼ˆæŒ‰æ–‡ä»¶åï¼‰===\n",
    "gpt_df.rename(columns={\"filename\": \"pdf_name\", \"report_year\": \"normalized_report_year\"}, inplace=True)\n",
    "merged = pd.merge(label_df, gpt_df, on=\"pdf_name\", how=\"inner\")\n",
    "\n",
    "# === 3. æ¸…æ´—æ–‡æœ¬ï¼šç©ºå€¼è½¬ç©ºä¸²ï¼Œå¤§å°å†™ã€ç©ºæ ¼ã€æ¢è¡Œå¤„ç† ===\n",
    "def clean_text(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    return str(s).strip().lower().replace(\"\\n\", \" \")\n",
    "\n",
    "merged[\"normalized_report_year\"] = merged[\"normalized_report_year\"].apply(clean_text)\n",
    "merged[\"chosen_rfyear\"] = merged[\"chosen_rfyear\"].apply(clean_text)\n",
    "\n",
    "# === 4. å®šä¹‰è¾…åŠ©å‡½æ•° ===\n",
    "import re\n",
    "\n",
    "def normalize_year_text(s):\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.lower()\n",
    "\n",
    "    # å»æ‰æ ‡ç‚¹ä¸å¤šä½™å­—ç¬¦\n",
    "    s = re.sub(r'[\\(\\)\\[\\],;:]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = re.sub(r'[^a-z0-9\\s\\-/â€“]', ' ', s)\n",
    "\n",
    "    # æ›¿æ¢ FY ç¼©å†™å’Œè¿æ¥ç¬¦ä¸º to\n",
    "    s = s.replace(\"â€“\", \"to\").replace(\"-\", \"to\").replace(\"/\", \"to\")\n",
    "    s = re.sub(r'\\bfy\\b', 'fiscal year', s)\n",
    "    s = re.sub(r'\\bfy(\\d{2})\\b', lambda m: f\"20{m.group(1)}\", s)  # fy20 â†’ 2020\n",
    "    s = re.sub(r'\\bfy\\s*(\\d{4})\\b', r'\\1', s)\n",
    "\n",
    "    # å±•å¼€ç¼©å†™å¹´ä»½å¦‚ 2022â€“23 â†’ 2022 to 2023\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s*to\\s*(\\d{2})\\b', lambda m: f\"{m.group(1)} to 20{m.group(2)}\", s)\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s*[\\-â€“]\\s*(\\d{2})\\b', lambda m: f\"{m.group(1)} to 20{m.group(2)}\", s)\n",
    "    s = re.sub(r'\\b(20\\d{2})[\\-â€“](20\\d{2})\\b', r'\\1 to \\2', s)\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s+to\\s+(20\\d{2})\\b', r'\\1 to \\2', s)\n",
    "\n",
    "    # æ¸…ç†åºæ•°è¯ï¼Œå¦‚ 1st, 2nd, 3rd\n",
    "    s = re.sub(r'\\b(\\d{1,2})(st|nd|rd|th)\\b', r'\\1', s)\n",
    "\n",
    "    # ç»Ÿä¸€æ—¥æœŸæ ¼å¼ï¼Œå¦‚ \"31 march 2023\" â†’ \"31 march 2023\"\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def extract_years(s):\n",
    "    return sorted(set(re.findall(r'\\b(20\\d{2}|19\\d{2})\\b', s)))\n",
    "\n",
    "def is_fuzzy_match(a, b):\n",
    "    a_norm = normalize_year_text(a)\n",
    "    b_norm = normalize_year_text(b)\n",
    "    if a_norm == b_norm:\n",
    "        return True\n",
    "    a_years = extract_years(a_norm)\n",
    "    b_years = extract_years(b_norm)\n",
    "    if not a_years or not b_years:\n",
    "        return False\n",
    "    # å¹´ä»½é›†åˆå®Œå…¨ä¸€è‡´ or åŒ…å«å…³ç³»\n",
    "    if set(a_years) == set(b_years):\n",
    "        return True\n",
    "    if len(a_years) == 1 and a_years[0] in b_years:\n",
    "        return True\n",
    "    if len(b_years) == 1 and b_years[0] in a_years:\n",
    "        return True\n",
    "    # ä»…ä¸€ä¸ªå¹´ä»½æ—¶è¿‘ä¼¼åˆ¤æ–­\n",
    "    if len(a_years) == 1 and len(b_years) == 1 and abs(int(a_years[0]) - int(b_years[0])) <= 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# === 5. åº”ç”¨æ¨¡ç³ŠåŒ¹é…å‡½æ•° ===\n",
    "merged[\"fuzzy_match\"] = merged.apply(lambda row: is_fuzzy_match(row[\"normalized_report_year\"], row[\"chosen_rfyear\"]), axis=1)\n",
    "\n",
    "# === 6. è®¡ç®—å‡†ç¡®ç‡ ===\n",
    "total = len(merged)\n",
    "correct = merged[\"fuzzy_match\"].sum()\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"âœ… åˆå¹¶æ ·æœ¬æ•°ï¼š{total}\")\n",
    "print(f\"âœ… åŒ¹é…æ­£ç¡®æ•°ï¼š{correct}\")\n",
    "print(f\"âœ… æ¨¡ç³ŠåŒ¹é…å‡†ç¡®ç‡ï¼ˆFuzzy Accuracyï¼‰ï¼š{accuracy:.2%}\")\n",
    "\n",
    "# === 7. ä¿å­˜ç»“æœ ===\n",
    "merged[[\"pdf_name\", \"chosen_rfyear\", \"normalized_report_year\", \"fuzzy_match\"]].to_csv(\"eval/year_comparison3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a793833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… number of combine samplesï¼š224\n",
      "âœ… correct matchesï¼š189\n",
      "âœ… Fuzzy Accuracyï¼š84.38%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === 1. åŠ è½½æ•°æ® ===\n",
    "gpt_df = pd.read_csv(\"results/extracted_report_years_formal1.csv\")  # GPTæå–ç»“æœ\n",
    "label_df = pd.read_excel(\"check/rfyear_annotation.xlsx\")            # æ ‡æ³¨ç»“æœ\n",
    "\n",
    "# === 2. é‡å‘½ååˆ—å¯¹é½ã€åˆå¹¶ï¼ˆæŒ‰æ–‡ä»¶åï¼‰===\n",
    "gpt_df.rename(columns={\"filename\": \"pdf_name\", \"report_year\": \"normalized_report_year\"}, inplace=True)\n",
    "merged = pd.merge(label_df, gpt_df, on=\"pdf_name\", how=\"inner\")\n",
    "\n",
    "# === 3. æ¸…æ´—æ–‡æœ¬ï¼šç©ºå€¼è½¬ç©ºä¸²ï¼Œå¤§å°å†™ã€ç©ºæ ¼ã€æ¢è¡Œå¤„ç† ===\n",
    "def clean_text(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    return str(s).strip().lower().replace(\"\\n\", \" \")\n",
    "\n",
    "merged[\"normalized_report_year\"] = merged[\"normalized_report_year\"].apply(clean_text)\n",
    "merged[\"chosen_rfyear\"] = merged[\"chosen_rfyear\"].apply(clean_text)\n",
    "\n",
    "# === 4. å®šä¹‰è¾…åŠ©å‡½æ•° ===\n",
    "import re\n",
    "\n",
    "def normalize_year_text(s):\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.lower().strip()\n",
    "    \n",
    "    # æ›¿æ¢å¸¸è§è¿å­—ç¬¦ä¸ºç»Ÿä¸€æ ¼å¼\n",
    "    s = s.replace(\"â€“\", \" to \").replace(\"-\", \" to \").replace(\"/\", \" to \")\n",
    "\n",
    "    # æ¸…é™¤æ— æ•ˆå­—ç¬¦\n",
    "    s = re.sub(r'[\\(\\)\\[\\],;:]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "\n",
    "    # fy ç¼©å†™å¤„ç†ï¼šfy2020 æˆ– fy 2020 â†’ 2020\n",
    "    s = re.sub(r'\\bfy\\s*(\\d{4})\\b', r'\\1', s)                   # fy 2020 â†’ 2020\n",
    "    s = re.sub(r'\\bfy\\s*(\\d{2})\\b', lambda m: f\"20{m.group(1)}\", s)  # fy 19 â†’ 2019\n",
    "\n",
    "    # åŒºé—´æ ¼å¼å¤„ç†ï¼š2020 to 21 â†’ 2020 to 2021\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s+to\\s+(\\d{2})\\b', lambda m: f\"{m.group(1)} to 20{m.group(2)}\", s)\n",
    "\n",
    "    # åŒºé—´æ ¼å¼å¤„ç†ï¼š2020â€“2021ã€2020-2021ã€fy2020 to 2021 â†’ 2020 to 2021\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s*to\\s*(20\\d{2})\\b', r'\\1 to \\2', s)\n",
    "    s = re.sub(r'\\b(20\\d{2})\\s+to\\s+20\\d{2}', r'\\g<0>', s)\n",
    "\n",
    "    # æ¸…ç†åºæ•°è¯\n",
    "    s = re.sub(r'\\b(\\d{1,2})(st|nd|rd|th)\\b', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "def extract_years(s):\n",
    "    return sorted(set(re.findall(r'\\b(20\\d{2}|19\\d{2})\\b', s)))\n",
    "\n",
    "def is_fuzzy_match(a, b):\n",
    "    a_norm = normalize_year_text(a)\n",
    "    b_norm = normalize_year_text(b)\n",
    "    if a_norm == b_norm:\n",
    "        return True\n",
    "\n",
    "    # æå–å¹´ä»½é›†åˆ\n",
    "    a_years = extract_years(a_norm)\n",
    "    b_years = extract_years(b_norm)\n",
    "\n",
    "    if not a_years or not b_years:\n",
    "        return False\n",
    "\n",
    "    # æ’åºåå†æ¯”è¾ƒ\n",
    "    a_sorted = sorted(set(a_years))\n",
    "    b_sorted = sorted(set(b_years))\n",
    "\n",
    "    # å®Œå…¨ä¸€è‡´æˆ–åŒ…å«\n",
    "    if a_sorted == b_sorted:\n",
    "        return True\n",
    "    if len(a_sorted) == 1 and a_sorted[0] in b_sorted:\n",
    "        return True\n",
    "    if len(b_sorted) == 1 and b_sorted[0] in a_sorted:\n",
    "        return True\n",
    "\n",
    "    # ä»…ä¸€ä¸ªå¹´ä»½æ—¶å…è®¸Â±1\n",
    "    if len(a_sorted) == 1 and len(b_sorted) == 1 and abs(int(a_sorted[0]) - int(b_sorted[0])) <= 1:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# === 5. åº”ç”¨æ¨¡ç³ŠåŒ¹é…å‡½æ•° ===\n",
    "merged[\"fuzzy_match\"] = merged.apply(lambda row: is_fuzzy_match(row[\"normalized_report_year\"], row[\"chosen_rfyear\"]), axis=1)\n",
    "\n",
    "# === 6. è®¡ç®—å‡†ç¡®ç‡ ===\n",
    "total = len(merged)\n",
    "correct = merged[\"fuzzy_match\"].sum()\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"âœ… number of combine samplesï¼š{total}\")\n",
    "print(f\"âœ… correct matchesï¼š{correct}\")\n",
    "print(f\"âœ… Fuzzy Accuracyï¼š{accuracy:.2%}\")\n",
    "\n",
    "# === 7. ä¿å­˜ç»“æœ ===\n",
    "merged[[\"pdf_name\", \"chosen_rfyear\", \"normalized_report_year\", \"fuzzy_match\"]].to_csv(\"eval/year_comparison3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231e7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irp_pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
